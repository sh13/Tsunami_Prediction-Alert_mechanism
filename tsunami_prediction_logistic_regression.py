# -*- coding: utf-8 -*-
"""Tsunami_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12B6yglvzhqvHFB3yNzOhnP5a0OsKVy2q
"""

#importing important modules
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mp
import seaborn as sb

#uploading_csv
from google.colab import files
uploaded=files.upload()

#framework_creation
import io
su=pd.read_csv(io.BytesIO(uploaded['sources.csv']))

dl=['COUNTRY','STATE/PROVINCE','LOCATION','LATITUDE','LONGITUDE']
su=su.drop(dl,axis=1)

su.reset_index(inplace=True)

su=su.fillna(su.mean(),inplace=True)

su['VALIDITY']=su['VALIDITY'].replace(['3'],1)
su['VALIDITY']=su['VALIDITY'].replace(['4'],2)

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
X = su.iloc[:,:-1]
y = su['VALIDITY']
scaler = StandardScaler()
Xn=scaler.fit_transform(X)
pca=PCA(n_components=20)
Xs=pca.fit_transform(Xn)
X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size = 0.33, random_state=42)

from sklearn import  linear_model
lr = linear_model.LogisticRegression()
lr.fit(X_train, y_train)
print("Accuracy",lr.score(X_test, y_test))

#creating the confusion matrix for validation of the model
y_predicted = np.array(lr.predict(X_test))
y_prob1=lr.predict_proba(X_test)[:,1]
y_prob2=lr.predict_proba(X_test)[:,2]

from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,y_predicted)

